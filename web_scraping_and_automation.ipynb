{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "web_scraping_and_automation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benvictoria21/pythonWebScraping/blob/master/web_scraping_and_automation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2llZfvSjksxF",
        "outputId": "9a0fee6d-e2ea-47fc-a369-ab9dfcf939e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "# YouTube Link:\n",
        "\n",
        "\n",
        "\n",
        "# Ensure that you have both beautifulsoup and requests installed:\n",
        "\n",
        "#   pip install beautifulsoup4\n",
        "\n",
        "#   pip install requests\n",
        "\n",
        "\n",
        "\n",
        "import requests\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "\n",
        "# Using the requests module, we use the \"get\" function\n",
        "\n",
        "# provided to access the webpage provided as an\n",
        "\n",
        "# argument to this function:\n",
        "\n",
        "result = requests.get(\"https://www.google.com/\")\n",
        "\n",
        "\n",
        "\n",
        "# To make sure that the website is accessible, we can\n",
        "\n",
        "# ensure that we obtain a 200 OK response to indicate\n",
        "\n",
        "# that the page is indeed present:\n",
        "\n",
        "print(result.status_code)\n",
        "\n",
        "\n",
        "\n",
        "# For other potential status codes you may encounter,\n",
        "\n",
        "# consult the following Wikipedia page:\n",
        "\n",
        "# https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\n",
        "\n",
        "\n",
        "\n",
        "# We can also check the HTTP header of the website to\n",
        "\n",
        "# verify that we have indeed accessed the correct page:\n",
        "\n",
        "print(result.headers)\n",
        "\n",
        "\n",
        "\n",
        "# For more information on HTTP headers and the information\n",
        "\n",
        "# one can obtain from them, you may consult:\n",
        "\n",
        "# https://en.wikipedia.org/wiki/List_of_HTTP_header_fields\n",
        "\n",
        "\n",
        "\n",
        "# Now, let us store the page content of the website accessed\n",
        "\n",
        "# from requests to a variable:\n",
        "\n",
        "src = result.content\n",
        "\n",
        "\n",
        "\n",
        "# Now that we have the page source stored, we will use the\n",
        "\n",
        "# BeautifulSoup module to parse and process the source.\n",
        "\n",
        "# To do so, we create a BeautifulSoup object based on the\n",
        "\n",
        "# source variable we created above:\n",
        "\n",
        "soup = BeautifulSoup(src, 'lxml')\n",
        "\n",
        "\n",
        "\n",
        "# Now that the page source has been processed via Beautifulsoup\n",
        "\n",
        "# we can access specific information directly from it. For instance,\n",
        "\n",
        "# say we want to see a list of all of the links on the page:\n",
        "\n",
        "links = soup.find_all(\"a\")\n",
        "\n",
        "print(links)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# Perhaps we just want to extract the link that has contains the text\n",
        "\n",
        "# \"About\" on the page instead of every link. We can use the built-in\n",
        "\n",
        "# \"text\" function to access the text content between the <a> </a>\n",
        "\n",
        "# tags.\n",
        "\n",
        "for link in links:\n",
        "\n",
        "    if \"About\" in link.text:\n",
        "\n",
        "        print(link)\n",
        "\n",
        "        print(link.attrs['href'])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "{'Date': 'Mon, 28 Sep 2020 16:36:53 GMT', 'Expires': '-1', 'Cache-Control': 'private, max-age=0', 'Content-Type': 'text/html; charset=ISO-8859-1', 'P3P': 'CP=\"This is not a P3P policy! See g.co/p3phelp for more info.\"', 'Content-Encoding': 'gzip', 'Server': 'gws', 'X-XSS-Protection': '0', 'X-Frame-Options': 'SAMEORIGIN', 'Set-Cookie': '1P_JAR=2020-09-28-16; expires=Wed, 28-Oct-2020 16:36:53 GMT; path=/; domain=.google.com; Secure, NID=204=g_ubSiluSGDOPWsNz0L4cmdA73os4ImiKocYsCmhV4NApd2BTLLHlettIjcPlCMkZdhGU9DTz8IpTTDoy-E_35qQeKORdtGmVFtqwejx7q0Vd01-Cdym9y7Gs4_SDrujSYqeJuFfOOMP_nhsvgXCJNH4wv7H2JJB6AD8C2zni1s; expires=Tue, 30-Mar-2021 16:36:53 GMT; path=/; domain=.google.com; HttpOnly', 'Transfer-Encoding': 'chunked'}\n",
            "[<a class=\"gb1\" href=\"https://www.google.com/imghp?hl=en&amp;tab=wi\">Images</a>, <a class=\"gb1\" href=\"https://maps.google.com/maps?hl=en&amp;tab=wl\">Maps</a>, <a class=\"gb1\" href=\"https://play.google.com/?hl=en&amp;tab=w8\">Play</a>, <a class=\"gb1\" href=\"https://www.youtube.com/?gl=US&amp;tab=w1\">YouTube</a>, <a class=\"gb1\" href=\"https://news.google.com/?tab=wn\">News</a>, <a class=\"gb1\" href=\"https://mail.google.com/mail/?tab=wm\">Gmail</a>, <a class=\"gb1\" href=\"https://drive.google.com/?tab=wo\">Drive</a>, <a class=\"gb1\" href=\"https://www.google.com/intl/en/about/products?tab=wh\" style=\"text-decoration:none\"><u>More</u> »</a>, <a class=\"gb4\" href=\"http://www.google.com/history/optout?hl=en\">Web History</a>, <a class=\"gb4\" href=\"/preferences?hl=en\">Settings</a>, <a class=\"gb4\" href=\"https://accounts.google.com/ServiceLogin?hl=en&amp;passive=true&amp;continue=https://www.google.com/&amp;ec=GAZAAQ\" id=\"gb_70\" target=\"_top\">Sign in</a>, <a href=\"/advanced_search?hl=en&amp;authuser=0\">Advanced search</a>, <a href=\"/intl/en/ads/\">Advertising Programs</a>, <a href=\"/services/\">Business Solutions</a>, <a href=\"/intl/en/about.html\">About Google</a>, <a href=\"/intl/en/policies/privacy/\">Privacy</a>, <a href=\"/intl/en/policies/terms/\">Terms</a>]\n",
            "\n",
            "\n",
            "<a href=\"/intl/en/about.html\">About Google</a>\n",
            "/intl/en/about.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMG61HInksxJ",
        "outputId": "0947a558-c060-4a33-d96f-273f17a2615e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# YouTube Link: https://www.youtube.com/watch?v=oDtLJEc5Ako\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "In this video, we will be going over BeautifulSoup objects, namely:\n",
        "\n",
        "    Tag, NavigableString, BeautifulSoup, and Comment\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# To keep things simple and also reproducible, consider the following HTML code\n",
        "\n",
        "html_doc = \"\"\"\n",
        "\n",
        "<html><head><title>The Dormouse's story</title></head>\n",
        "\n",
        "<body>\n",
        "\n",
        "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
        "\n",
        "\n",
        "\n",
        "<p class=\"story\">Once upon a time there were three little sisters; their names:\n",
        "\n",
        "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
        "\n",
        "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
        "\n",
        "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
        "\n",
        "and they lived at the bottom of a well.</p>\n",
        "\n",
        "\n",
        "\n",
        "<p class=\"story\">...</p>\n",
        "\n",
        "\n",
        "\n",
        "<b class=\"boldest\">Extremely bold</b>\n",
        "\n",
        "<blockquote class=\"boldest\">Extremely bold</blockquote>\n",
        "\n",
        "<b id=\"1\">Test 1</b>\n",
        "\n",
        "<b another-attribute=\"1\" id=\"verybold\">Test 2</b>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open('index.html', 'w') as f:\n",
        "\n",
        "    f.write(html_doc)\n",
        "\n",
        "\n",
        "\n",
        "soup = BeautifulSoup(html_doc, \"lxml\")\n",
        "\n",
        "\n",
        "\n",
        "#print(soup.prettify())\n",
        "\n",
        "\n",
        "\n",
        "# Tag:\n",
        "\n",
        "\n",
        "\n",
        "# Finds the first occurrence of usage for a \"b\"\n",
        "\n",
        "# bold tag.\n",
        "\n",
        "#print(soup.b)\n",
        "\n",
        "\n",
        "\n",
        "# The \"find\" function also does the same, where it\n",
        "\n",
        "# only finds the first occurrence in the HTML doc\n",
        "\n",
        "# of a tag with \"b\".\n",
        "\n",
        "#print(soup.find('b'))\n",
        "\n",
        "\n",
        "\n",
        "# If we want to find all of the elements on the page\n",
        "\n",
        "# with the \"b\" tag, we can use the \"find_all\" function.\n",
        "\n",
        "#print(soup.find_all('b'))\n",
        "\n",
        "\n",
        "\n",
        "# Name:\n",
        "\n",
        "\n",
        "\n",
        "# This gives the name of the tag. In this case, the \n",
        "\n",
        "# tag name is \"b\".\n",
        "\n",
        "#print(soup.b.name)\n",
        "\n",
        "\n",
        "\n",
        "# We can alter the name and have that reflected in the\n",
        "\n",
        "# source. For instance:\n",
        "\n",
        "#tag = soup.b\n",
        "\n",
        "#print(tag)\n",
        "\n",
        "#tag.name = \"blockquote\"\n",
        "\n",
        "#print(tag)\n",
        "\n",
        "\n",
        "\n",
        "# Attributes:\n",
        "\n",
        "\n",
        "\n",
        "#tag = soup.find_all('b')[2]\n",
        "\n",
        "#print(tag)\n",
        "\n",
        "\n",
        "\n",
        "# This specific tag has the attribute \"id\", which\n",
        "\n",
        "# can be accessed like so:\n",
        "\n",
        "#print(tag['id'])\n",
        "\n",
        "\n",
        "\n",
        "#tag = soup.find_all('b')[3]\n",
        "\n",
        "#print(tag)\n",
        "\n",
        "\n",
        "\n",
        "# We can even access multiple attributes that are\n",
        "\n",
        "# non-standard HTML attributes:\n",
        "\n",
        "#print(tag['id'])\n",
        "\n",
        "#print(tag['another-attribute'])\n",
        "\n",
        "\n",
        "\n",
        "# If we want to see all attributes, we can access them\n",
        "\n",
        "# as a dictionary object:\n",
        "\n",
        "#tag = soup.find_all('b')[3]\n",
        "\n",
        "#print(tag)\n",
        "\n",
        "\n",
        "\n",
        "#print(tag.attrs)\n",
        "\n",
        "\n",
        "\n",
        "# These properties are mutable, and we can alter them\n",
        "\n",
        "# in the following manner.\n",
        "\n",
        "#print(tag)\n",
        "\n",
        "#tag['another-attribute'] = 2\n",
        "\n",
        "#print(tag)\n",
        "\n",
        "\n",
        "\n",
        "# We can also use Python's del command for lists to\n",
        "\n",
        "# remove attributes:\n",
        "\n",
        "#del tag['id']\n",
        "\n",
        "#del tag['another-attribute']\n",
        "\n",
        "#print(tag)\n",
        "\n",
        "\n",
        "\n",
        "# Multi-valued Attributes\n",
        "\n",
        "tag = soup.find_all('b')[3]\n",
        "\n",
        "print(tag)\n",
        "\n",
        "print(tag.string)\n",
        "\n",
        "\n",
        "\n",
        "# We can use the \"replace_with\" function to replace\n",
        "\n",
        "# the content of the string with something different:\n",
        "\n",
        "tag.string.replace_with(\"This is another string\")\n",
        "\n",
        "print(tag)\n",
        "\n",
        "\n",
        "\n",
        "# NavigableString\n",
        "\n",
        "\n",
        "\n",
        "# BeautifulSoup\n",
        "\n",
        "\n",
        "\n",
        "# Comments"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<b another-attribute=\"1\" id=\"verybold\">Test 2</b>\n",
            "Test 2\n",
            "<b another-attribute=\"1\" id=\"verybold\">This is another string</b>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2am7j5hAksxM",
        "outputId": "ce766fda-01c8-4a2b-a82d-9ac3c4b8c21b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Let's obtain the links from the following website:\n",
        "\n",
        "# https://www.whitehouse.gov/briefings-statements/\n",
        "\n",
        "\n",
        "\n",
        "# One of the things this website consists of is records of presidential\n",
        "\n",
        "# briefings and statements.\n",
        "\n",
        "\n",
        "\n",
        "# Goal: Extract all of the links on the page that point to the \n",
        "\n",
        "# briefings and statements.\n",
        "\n",
        "\n",
        "\n",
        "import requests\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "\n",
        "result = requests.get(\"https://www.whitehouse.gov/briefings-statements/\")\n",
        "\n",
        "src = result.content\n",
        "\n",
        "soup = BeautifulSoup(src, 'lxml')\n",
        "\n",
        "\n",
        "\n",
        "urls = []\n",
        "\n",
        "for h2_tag in soup.find_all('h2'):\n",
        "\n",
        "    a_tag = h2_tag.find('a')\n",
        "\n",
        "    urls.append(a_tag.attrs['href'])\n",
        "\n",
        "\n",
        "\n",
        "print(urls)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['https://www.whitehouse.gov/briefings-statements/remarks-president-trump-press-briefing-september-27-2020/', 'https://www.whitehouse.gov/briefings-statements/wtas-support-president-donald-j-trumps-nomination-judge-amy-coney-barrett-supreme-court/', 'https://www.whitehouse.gov/briefings-statements/presidential-message-yom-kippur-2020/', 'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-marine-one-departure-092620/', 'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-announcing-nominee-associate-justice-supreme-court-united-states/', 'https://www.whitehouse.gov/briefings-statements/judge-amy-coney-barretts-exceptional-legal-experience-expertise-judicial-record-make-right-choice-serve-supreme-court/', 'https://www.whitehouse.gov/briefings-statements/presidential-message-national-day-prayer-return-2020/', 'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-air-force-one-arrival-joint-base-andrews-md/', 'https://www.whitehouse.gov/briefings-statements/second-lady-karen-pence-highlights-veteran-beekeeping-program-lansing-michigan/', 'https://www.whitehouse.gov/briefings-statements/statement-press-secretary-regarding-united-states-united-kingdom-special-relationship-economic-working-group-meeting/']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWD4yGqEksxP",
        "outputId": "eedcb403-c218-4a55-e5e3-19e6c9c7cc9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# YouTube Link:\n",
        "\n",
        "\n",
        "\n",
        "# Let's obtain the links from the following website:\n",
        "\n",
        "# https://www.whitehouse.gov/briefings-statements/\n",
        "\n",
        "\n",
        "\n",
        "# One of the things this website consists of is records of presidential\n",
        "\n",
        "# briefings and statements.\n",
        "\n",
        "\n",
        "\n",
        "# Goal: Extract all of the links on the page that point to the \n",
        "\n",
        "# briefings and statements.\n",
        "\n",
        "\n",
        "\n",
        "import requests\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "\n",
        "result = requests.get(\"https://www.whitehouse.gov/briefings-statements/\")\n",
        "\n",
        "src = result.content\n",
        "\n",
        "soup = BeautifulSoup(src, 'lxml')\n",
        "\n",
        "\n",
        "\n",
        "urls = []\n",
        "\n",
        "for h2_tag in soup.find_all('h2'):\n",
        "\n",
        "    a_tag = h2_tag.find('a')\n",
        "\n",
        "    urls.append(a_tag.attrs['href'])\n",
        "\n",
        "\n",
        "\n",
        "print(urls)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['https://www.whitehouse.gov/briefings-statements/remarks-president-trump-press-briefing-september-27-2020/', 'https://www.whitehouse.gov/briefings-statements/wtas-support-president-donald-j-trumps-nomination-judge-amy-coney-barrett-supreme-court/', 'https://www.whitehouse.gov/briefings-statements/presidential-message-yom-kippur-2020/', 'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-marine-one-departure-092620/', 'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-announcing-nominee-associate-justice-supreme-court-united-states/', 'https://www.whitehouse.gov/briefings-statements/judge-amy-coney-barretts-exceptional-legal-experience-expertise-judicial-record-make-right-choice-serve-supreme-court/', 'https://www.whitehouse.gov/briefings-statements/presidential-message-national-day-prayer-return-2020/', 'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-air-force-one-arrival-joint-base-andrews-md/', 'https://www.whitehouse.gov/briefings-statements/second-lady-karen-pence-highlights-veteran-beekeeping-program-lansing-michigan/', 'https://www.whitehouse.gov/briefings-statements/statement-press-secretary-regarding-united-states-united-kingdom-special-relationship-economic-working-group-meeting/']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}